> Perhaps it would be helpful to show some kind of indicator in the top right corner of the html
 page as to how many more inquiries are allowed within the next time frame (hour?). Also, since
batching works, do we need to find a way to enable the user to "drill down" into topics until
they have a full batch and then go retrieve the data and update the html page? plan only. What
do we need to do to maximize our data retrieval without triggering rate limits?

● Excellent strategic thinking! Let me create a comprehensive plan for maximizing data retrieval
   while staying under rate limits.

  Plan: Rate Limit Management & Batch Optimization

  ---
  1. Rate Limit Indicator UI (Top-Right Corner)

  Design:

  ┌─────────────────────────────────────────┐
  │ Agent 0 Dashboard          [API: 4/10] │  ← Top right corner
  └─────────────────────────────────────────┘

  Display Elements:

  - Current usage: "4/10 calls used"
  - Time to reset: "Resets in 42 minutes"
  - Color coding:
    - Green (0-5 calls): Safe zone
    - Yellow (6-8 calls): Caution zone
    - Red (9-10 calls): Near limit
  - Cache indicator: "✓ 3 cached topics available"

  Data Source:

  - Track API calls in session storage (browser localStorage)
  - Record timestamp of each batch request
  - Calculate "calls used in last 60 minutes"
  - Show cache hit/miss ratio

  Implementation:

  - Add to dashboard HTML header
  - JavaScript counter that updates in real-time
  - Persist across page refreshes using localStorage

  ---
  2. Batch-Optimized Drill-Down Workflow

  Current Problem:

  User selects 1 topic → Drill down → 20 sub-topics = 4 API calls immediately

  Optimized Solution: "Queue & Batch" Pattern

  User Flow:
  1. User selects topic → Add to drill-down queue
  2. User selects 2nd topic → Add to queue (now 2 topics)
  3. User selects 3rd topic → Add to queue (now 3 topics)
  4. User clicks "Batch Drill-Down (3 topics)" button
  5. System generates 60 sub-topics (3 topics × 20 each)
  6. System batches into 12 requests (60 ÷ 5 = 12 batches)
  7. Execute with 12-second delays = ~2.5 minutes total

  UI Changes:

  New "Drill-Down Queue" Panel:
  ┌──────────────────────────────────────────┐
  │ Drill-Down Queue (2 topics)              │
  │ ─────────────────────────────────────────│
  │ ✓ romance novels                    [×]  │
  │ ✓ productivity apps                 [×]  │
  │                                          │
  │ [+ Add Topic]  [Batch Drill-Down (2×20)] │
  │                                          │
  │ Will generate: 40 sub-topics            │
  │ API calls needed: 8 batches (~2 min)    │
  │ Remaining capacity: 2/10 calls          │
  └──────────────────────────────────────────┘

  Smart Batch Button:
  - Green: "Safe to drill-down (under limit)"
  - Yellow: "Caution - will use 8/10 calls"
  - Red: "WAIT - exceeds rate limit. Try again in 30 min"

  ---
  3. Maximum Data Retrieval Strategy

  Optimization Techniques:

  A. Intelligent Queueing System

  Priority Queue Logic:
  1. User adds topics to queue (no API calls yet)
  2. System calculates optimal batch size based on:
     - Current API usage (e.g., 4/10 used)
     - Available capacity (6 calls remaining)
     - Cache hits (cached topics = free!)
  3. System suggests: "You can drill-down 1 topic safely (4 calls)"
  4. User confirms → Execute batches

  B. Cache-First Strategy

  Before ANY API call:
  1. Check cache for topic + all sub-topics
  2. If 100% cached → Instant results (0 API calls)
  3. If partial cached (e.g., 15/20 sub-topics cached):
     - Use cached data
     - Only query 5 uncached sub-topics (1 batch instead of 4!)

  C. Progressive Loading

  Instead of: Wait 2 minutes → Show all 20 sub-topics
  Do this: Show results as each batch completes

  Batch 1 (5 sub-topics) → 12s → Update UI ✓
  Batch 2 (5 sub-topics) → 12s → Update UI ✓
  Batch 3 (5 sub-topics) → 12s → Update UI ✓
  Batch 4 (5 sub-topics) → 12s → Update UI ✓

  User sees progress instead of loading spinner!

  D. Smart Timing Window

  Rate Limit Window Tracker:
  - Track WHEN each API call was made
  - Calculate rolling 60-minute window
  - Oldest call drops off after 60 minutes
  - Auto-refresh capacity indicator

  Example:
  10:00 AM - Made 8 calls
  10:30 AM - User wants to drill-down (4 more calls)
  System says: "WAIT 30 minutes (3/8 calls expire at 11:00 AM)"
  11:01 AM - Capacity restored to 7/10 available

  ---
  4. Session Management & Multi-Session Strategy

  Problem:

  User researches 10 topics → Drills down 2 topics → Hits limit → Blocked for 1 hour

  Solution: Session Awareness

  Session Tracker UI:
  ┌────────────────────────────────────────┐
  │ Today's Research Summary               │
  │ ──────────────────────────────────────│
  │ Topics researched: 15                  │
  │ Sub-topics discovered: 40              │
  │ API calls used today: 24               │
  │ Cache hits: 12 (saved 12 API calls!)  │
  │                                        │
  │ Next available slot: 2:30 PM           │
  └────────────────────────────────────────┘

  Multi-Session Workflow:

  Morning Session (9:00 AM):
  - Research 10 topics (2 batches) ✓
  - Drill-down 1 topic (4 batches) ✓
  - Total: 6/10 calls used

  Afternoon Session (10:30 AM):
  - Cache hits on morning topics (0 calls) ✓
  - Drill-down 1 more topic (4 batches) ✓
  - Total: 10/10 calls used → WAIT

  Evening Session (11:30 AM):
  - Morning calls expired (6 calls freed)
  - Available: 6/10 calls
  - Drill-down 1 topic (4 batches) ✓

  ---
  5. Advanced: Predictive Pre-Caching

  Concept:

  Predict which sub-topics user will want BEFORE they click "drill-down"

  Strategy:

  When user hovers over "romance novels":
  1. System predicts likely sub-topics:
     - "historical romance"
     - "contemporary romance"
     - "paranormal romance"
     - etc.
  2. If API capacity available (< 5/10 used):
     - Pre-fetch 1 batch (5 sub-topics) in background
     - Cache results
     - When user clicks drill-down → Already cached!
  3. If capacity limited (> 8/10 used):
     - Skip pre-fetching

  Benefit: Feels instant even without hitting cache

  ---
  6. Implementation Priority

  Phase 1: Essential (1-2 hours)

  1. ✅ Rate limit indicator (top-right corner)
  2. ✅ localStorage tracking of API calls
  3. ✅ Visual warning when near limit

  Phase 2: Batch Optimization (2-3 hours)

  4. ✅ Drill-down queue system
  5. ✅ Batch button with capacity check
  6. ✅ Progressive loading UI

  Phase 3: Advanced (future)

  7. ⏸️ Predictive pre-caching
  8. ⏸️ Multi-session dashboard
  9. ⏸️ Scheduled batch jobs

  ---
  7. Maximum Capacity Calculation

  With Current Implementation:

  | Scenario           | Topics | Sub-Topics | API Calls | Time | Feasible?     |
  |--------------------|--------|------------|-----------|------|---------------|
  | Initial research   | 10     | 0          | 2         | 24s  | ✅ Yes         |
  | Drill-down 1 topic | 1      | 20         | 4         | 48s  | ✅ Yes         |
  | Total Session      | 10     | 20         | 6         | 72s  | ✅ Under limit |
  | Next hour          | 10     | 20         | 0         | 0s   | ✅ All cached! |

  With Queue System:

  | Scenario         | Topics | Sub-Topics | API Calls | Time | Feasible?                |
  |------------------|--------|------------|-----------|------|--------------------------|
  | Research 10      | 10     | 0          | 2         | 24s  | ✅ Yes                    |
  | Queue 2 topics   | 2      | 40         | 8         | 96s  | ✅ Yes                    |
  | Total Session    | 10     | 40         | 10        | 120s | ✅ Exactly at limit       |
  | Wait 60 min      | -      | -          | -         | -    | Reset                    |
  | Research 10 more | 20     | 40         | 2         | 24s  | ✅ Yes (first 10 cached!) |

  ---
  Key Insights:

  1. Batching is game-changer: 10 topics = 2 calls (was 10 calls)
  2. Caching is force-multiplier: 2nd run = 0 calls (was 10 calls)
  3. Queue system enables bulk: 2 drill-downs at once = efficient
  4. Visual indicator prevents errors: User sees limit before hitting it